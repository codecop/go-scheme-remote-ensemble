

Data Flow / Algorithm / Steps / 
- File name
- read file -> string
- string -> scan -> list of tokens (type/value)
    - types of tokens: open brackets, close brackets, names (function, assignment, â€¦), numbers, strings/quotes, #t, #f, symbols
- list of tokens -> tree
    - Content: tokens
    - approach: new children if reach open bracket
    - check for enough brackets
- language semantic checks, type checks
    - number of arguments, string function need to be string
- Tree -> parsing/error handling and interpreting
- -> execution -> result
- result -> print

Paul -> Corinna -> Pavel -> Sergiu -> Gery -> 

Design (what goes where)
- inputs
    - file path/name/extension
    - file
        - what do expect in file
        - what to do about whitespace, delimiters, new line, comments (;)
        - encoding of file (utf8)
- output 
    - text of all evaluated expressions in the file
    - one line per expression
    - if error, see line number, description

Potential components
- file handling
- tokenizing 
- parsing
- semantic checks (checks for all levels (e.g., tokens, tree))
- error collector
    - line numbers
    
- execution
- display
- main component

- Discussion
    - checks in component or in each component
    
- move input into tokenizer
    - write in one file
- analyse syntax
- build up a tree
- separate into packages    


Tokenizer
- input:
    - string
    - cleaned
- output:
    - list of tokens
        - what type is it? (e.g. slice, linked list, interface, struct)
        - brackets, identifiers (func names, variables), values (number, "string", boolean #t#f, other literals)
        - ignore symbols 
        - (extend later if necessary)
    - token (struct, interface)
        - value (e.g. identifier -> string, number -> number, bracket -> nil)
        - type
